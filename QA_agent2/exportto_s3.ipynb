{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be135c9",
   "metadata": {},
   "source": [
    "# Export to S3 Function\n",
    "\n",
    "This notebook contains a function to export all files and folders from a SageMaker source directory to an S3 bucket folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e776073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_directory_to_s3(bucket_name, s3_folder, source_directory, aws_access_key_id=None, aws_secret_access_key=None, region_name='us-east-1'):\n",
    "    \"\"\"\n",
    "    Export all files and folders from a local directory to an S3 bucket folder.\n",
    "    Overwrites existing files and adds new files.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name of the S3 bucket\n",
    "        s3_folder (str): Destination folder in S3 bucket\n",
    "        source_directory (str): Local source directory path\n",
    "        aws_access_key_id (str, optional): AWS access key ID. If None, uses default credentials\n",
    "        aws_secret_access_key (str, optional): AWS secret access key. If None, uses default credentials\n",
    "        region_name (str): AWS region name (default: 'us-east-1')\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create S3 client\n",
    "        if aws_access_key_id and aws_secret_access_key:\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=aws_access_key_id,\n",
    "                aws_secret_access_key=aws_secret_access_key,\n",
    "                region_name=region_name\n",
    "            )\n",
    "        else:\n",
    "            # Use default credentials (from ~/.aws/credentials, IAM role, etc.)\n",
    "            s3_client = boto3.client('s3', region_name=region_name)\n",
    "        \n",
    "        # Ensure source directory exists\n",
    "        source_path = Path(source_directory)\n",
    "        if not source_path.exists():\n",
    "            logger.error(f\"Source directory '{source_directory}' does not exist\")\n",
    "            return False\n",
    "        \n",
    "        # Ensure s3_folder doesn't start with '/' and ends with '/'\n",
    "        if s3_folder.startswith('/'):\n",
    "            s3_folder = s3_folder[1:]\n",
    "        if not s3_folder.endswith('/'):\n",
    "            s3_folder += '/'\n",
    "        \n",
    "        logger.info(f\"Starting upload from '{source_directory}' to 's3://{bucket_name}/{s3_folder}'\")\n",
    "        \n",
    "        uploaded_count = 0\n",
    "        \n",
    "        # Walk through all files and directories in the source directory\n",
    "        for root, dirs, files in os.walk(source_directory):\n",
    "            # Process all files in current directory\n",
    "            for file in files:\n",
    "                # Get full local file path\n",
    "                local_file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Calculate relative path from source directory\n",
    "                relative_path = os.path.relpath(local_file_path, source_directory)\n",
    "                \n",
    "                # Create S3 key (object path)\n",
    "                s3_key = s3_folder + relative_path.replace('\\\\', '/')  # Use forward slashes for S3\n",
    "                \n",
    "                try:\n",
    "                    # Upload file to S3\n",
    "                    s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "                    uploaded_count += 1\n",
    "                    logger.info(f\"Uploaded: {relative_path} -> s3://{bucket_name}/{s3_key}\")\n",
    "                    \n",
    "                except ClientError as e:\n",
    "                    logger.error(f\"Error uploading {relative_path}: {e}\")\n",
    "                    continue\n",
    "                except FileNotFoundError:\n",
    "                    logger.error(f\"File not found: {local_file_path}\")\n",
    "                    continue\n",
    "        \n",
    "        if uploaded_count == 0:\n",
    "            logger.warning(f\"No files found in directory '{source_directory}'\")\n",
    "        else:\n",
    "            logger.info(f\"Successfully uploaded {uploaded_count} files to s3://{bucket_name}/{s3_folder}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        logger.error(\"AWS credentials not found. Please configure your credentials.\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'NoSuchBucket':\n",
    "            logger.error(f\"Bucket '{bucket_name}' does not exist\")\n",
    "        elif error_code == 'AccessDenied':\n",
    "            logger.error(f\"Access denied to bucket '{bucket_name}'. Check your permissions.\")\n",
    "        else:\n",
    "            logger.error(f\"AWS error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b998750",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Here's how to use the function with your specific parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b697179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting upload from '/home/sagemaker-user/codespace/nlp_assignments/QA_agent2/' to 's3://qa-agent-files/results/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Uploaded: HW4_baseline.ipynb -> s3://qa-agent-files/results/HW4_baseline.ipynb\n",
      "INFO:__main__:Uploaded: countries_with_languages.tsv -> s3://qa-agent-files/results/countries_with_languages.tsv\n",
      "INFO:__main__:Uploaded: example.jsonl -> s3://qa-agent-files/results/example.jsonl\n",
      "INFO:__main__:Uploaded: hw4_handout.pdf -> s3://qa-agent-files/results/hw4_handout.pdf\n",
      "INFO:__main__:Uploaded: private.jsonl -> s3://qa-agent-files/results/private.jsonl\n",
      "INFO:__main__:Uploaded: public.jsonl -> s3://qa-agent-files/results/public.jsonl\n",
      "INFO:__main__:Uploaded: example_output.txt -> s3://qa-agent-files/results/example_output.txt\n",
      "INFO:__main__:Uploaded: import_from_s3.ipynb -> s3://qa-agent-files/results/import_from_s3.ipynb\n",
      "INFO:__main__:Uploaded: .gitignore -> s3://qa-agent-files/results/.gitignore\n",
      "INFO:__main__:Uploaded: test.json -> s3://qa-agent-files/results/test.json\n",
      "INFO:__main__:Uploaded: build_index.py -> s3://qa-agent-files/results/build_index.py\n",
      "INFO:__main__:Uploaded: leaderboard.txt -> s3://qa-agent-files/results/leaderboard.txt\n",
      "INFO:__main__:Uploaded: exportto_s3.ipynb -> s3://qa-agent-files/results/exportto_s3.ipynb\n",
      "INFO:__main__:Uploaded: data/recipes.csv -> s3://qa-agent-files/results/data/recipes.csv\n",
      "INFO:__main__:Uploaded: data/train.json -> s3://qa-agent-files/results/data/train.json\n",
      "INFO:__main__:Uploaded: models/faiss_index/index.faiss -> s3://qa-agent-files/results/models/faiss_index/index.faiss\n",
      "INFO:__main__:Uploaded: models/faiss_index/index.pkl -> s3://qa-agent-files/results/models/faiss_index/index.pkl\n",
      "INFO:__main__:Uploaded: rag_system/_init_.py -> s3://qa-agent-files/results/rag_system/_init_.py\n",
      "INFO:__main__:Uploaded: rag_system/tools.py -> s3://qa-agent-files/results/rag_system/tools.py\n",
      "INFO:__main__:Uploaded: rag_system/rag_pipeline.py -> s3://qa-agent-files/results/rag_system/rag_pipeline.py\n",
      "INFO:__main__:Uploaded: rag_system/__pycache__/rag_pipeline.cpython-312.pyc -> s3://qa-agent-files/results/rag_system/__pycache__/rag_pipeline.cpython-312.pyc\n",
      "INFO:__main__:Uploaded: .vscode/settings.json -> s3://qa-agent-files/results/.vscode/settings.json\n",
      "INFO:__main__:Successfully uploaded 22 files to s3://qa-agent-files/results/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported files from /home/sagemaker-user/codespace/nlp_assignments/QA_agent2/ to s3://qa-agent-files/results/\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Using your specific parameters (with default AWS credentials)\n",
    "bucket_name = \"qa-agent-files\"\n",
    "s3_folder = \"results\"\n",
    "source_directory = \"/home/sagemaker-user/codespace/nlp_assignments/QA_agent2/\"\n",
    "\n",
    "success = export_directory_to_s3(bucket_name, s3_folder, source_directory)\n",
    "if success:\n",
    "    print(f\"Successfully exported files from {source_directory} to s3://{bucket_name}/{s3_folder}/\")\n",
    "else:\n",
    "    print(\"Export failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3473e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 2: Using explicit credentials (if needed)\n",
    "# bucket_name = \"sagemaker-studio-940544301691-64iyil0o5a\"\n",
    "# s3_folder = \"nlp_assignment\"\n",
    "# source_directory = \"/home/sagemaker-user/codespace/nlp_assignments\"\n",
    "# access_key = \"YOUR_ACCESS_KEY_ID\"  # Replace with your access key\n",
    "# secret_key = \"YOUR_SECRET_ACCESS_KEY\"  # Replace with your secret key\n",
    "\n",
    "# success = export_directory_to_s3(\n",
    "#     bucket_name=bucket_name,\n",
    "#     s3_folder=s3_folder,\n",
    "#     source_directory=source_directory,\n",
    "#     aws_access_key_id=access_key,\n",
    "#     aws_secret_access_key=secret_key,\n",
    "#     region_name='us-east-1'  # Adjust region if needed\n",
    "# )\n",
    "\n",
    "# if success:\n",
    "#     print(f\"Successfully exported files from {source_directory} to s3://{bucket_name}/{s3_folder}/\")\n",
    "# else:\n",
    "#     print(\"Export failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fb65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example 3: Quick function for your specific use case\n",
    "# def export_nlp_assignments():\n",
    "#     \"\"\"Quick function to export your NLP assignments to S3\"\"\"\n",
    "#     return export_directory_to_s3(\n",
    "#         bucket_name=\"sagemaker-studio-940544301691-64iyil0o5a\",\n",
    "#         s3_folder=\"nlp_assignment\",\n",
    "#         source_directory=\"/home/sagemaker-user/codespace/nlp_assignments\"\n",
    "#     )\n",
    "\n",
    "# # Simply call this function to export your files\n",
    "# # success = export_nlp_assignments()\n",
    "# # print(\"Export completed!\" if success else \"Export failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc07d9",
   "metadata": {},
   "source": [
    "## Function Features\n",
    "\n",
    "- **Complete directory upload**: Uploads all files and subfolders recursively\n",
    "- **Overwrites existing files**: Files with the same name will be replaced\n",
    "- **Preserves folder structure**: Maintains the original directory structure in S3\n",
    "- **Handles large files**: Uses efficient file upload methods\n",
    "- **Error handling**: Comprehensive error handling and logging\n",
    "- **Flexible authentication**: Supports both default AWS credentials and explicit credentials\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **AWS credentials configured** in one of these ways:\n",
    "   - IAM role (recommended for SageMaker)\n",
    "   - AWS credentials file (`~/.aws/credentials`)\n",
    "   - Environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`)\n",
    "   - Or pass credentials directly to the function\n",
    "\n",
    "2. **Required Python packages** (usually pre-installed in SageMaker):\n",
    "   ```bash\n",
    "   pip install boto3\n",
    "   ```\n",
    "\n",
    "3. **S3 permissions**: Write access to the specified bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fab682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Universal Environment",
   "language": "python",
   "name": "ml-universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
