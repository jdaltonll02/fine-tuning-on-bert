{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f3fe1f",
   "metadata": {},
   "source": [
    "# HW4: QA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c91ec1",
   "metadata": {},
   "source": [
    "## Dependencies and LLM Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42689839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==1.0.5\n",
    "# !pip install langchain-core\n",
    "# !pip install langchain-community\n",
    "# !pip install faiss-cpu\n",
    "# !pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of countries we are using (with their official languages)\n",
    "# Feel free to use it in your code\n",
    "list_of_countries = {}\n",
    "with open(\"countries_with_languages.tsv\", \"r\"  ) as f:\n",
    "    for line in f.readlines():\n",
    "        country, langs = line.strip().split(\"\\t\")\n",
    "        list_of_countries[country] = langs.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06a7d",
   "metadata": {},
   "source": [
    "### Choice 1: OpenAI API\n",
    "\n",
    "The notebook's implementation is based on this.\n",
    "Feel free to change the model, and please keep track of your usage on the \"Usage\" page on [LiteLLM API webpage](https://ai-gateway.andrew.cmu.edu/ui/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65e4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5add67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import getpass, os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "openai_model_id = \"gpt-4o-mini-2024-07-18\"\n",
    "openai_embmodel_id = \"azure/text-embedding-3-small\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=openai_model_id,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://ai-gateway.andrew.cmu.edu/\"\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=openai_embmodel_id,\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    base_url='https://ai-gateway.andrew.cmu.edu/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b00ca",
   "metadata": {},
   "source": [
    "### Choice 2: Hugging Face Models\n",
    "\n",
    "You may also use Hugging Face models without API credits if you have available GPU resource. You might have to the change prompt templates according to your model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1bc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface text-generation transformers google-search-results \n",
    "# !pip install numexpr langchainhub sentencepiece sentence-transformers jinja2 bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf0fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 14:56:20.708736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764341780.725575   47552 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764341780.730872   47552 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-28 14:56:20.748247: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import getpass, os\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")\n",
    "hgf_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "hgf_embmodel_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "hgf_model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=hgf_model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    ),\n",
    ")\n",
    "hgf_llm = ChatHuggingFace(llm=hgf_model)\n",
    "hgf_embeddings = HuggingFaceEmbeddings(model_name=hgf_embmodel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a37cbe",
   "metadata": {},
   "source": [
    "## Handling different type of questions\n",
    "\n",
    "Implement the answer formatting and extraction for each question type. You may change the prompt to fit your processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e242dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a4262",
   "metadata": {},
   "source": [
    "### ðŸ—ºï¸Global Trekker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bce1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trekker_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in world knowledge.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given the following paragraph, guess the most likely country and city. Answer in the format of [country], [city].\"},\n",
    "]\n",
    "global_trekker = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85342fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_trekker_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and city from the response\n",
    "    # return country, city\n",
    "    import re\n",
    "    \n",
    "    # Look for pattern [country], [city] or variations\n",
    "    # Try to find content within brackets first\n",
    "    bracket_pattern = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(bracket_pattern, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        city = match.group(2).strip()\n",
    "        return country, city\n",
    "    \n",
    "    # Fallback: look for two items separated by comma\n",
    "    # Often LLMs will say something like \"United States, Pittsburgh\"\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Look for comma-separated values\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                city = parts[1].strip().strip('[]\"\\'')\n",
    "                # Clean up common prefixes\n",
    "                for prefix in ['The answer is', 'Answer:', 'Location:', 'Country:', 'City:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    city = city.replace(prefix, '').strip()\n",
    "                return country, city\n",
    "    \n",
    "    # Last resort: return empty strings\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5210d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AAA', 'BBB')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run your extration function before using it in the main loop!\n",
    "extract_global_trekker_answer(\"AAA, BBB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ab9c1",
   "metadata": {},
   "source": [
    "### ðŸ½ï¸Culinary Detective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a359380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "def gather_recipe_data(kaggledataset: str) -> list[str]:\n",
    "    dataset_path = kagglehub.dataset_download(kaggledataset)\n",
    "    df = pd.read_csv(f\"{dataset_path}/Receipes from around the world.csv\", encoding='latin-1')\n",
    "    \n",
    "    # Process the dataframe to list of text entries for retrieval\n",
    "    # Format each recipe as structured text for better retrieval\n",
    "    recipes = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a readable text representation of each recipe\n",
    "        recipe_parts = []\n",
    "        for col in df.columns:\n",
    "            value = row[col]\n",
    "            # Skip NaN values and format nicely\n",
    "            if pd.notna(value) and str(value).strip():\n",
    "                recipe_parts.append(f\"{col}: {value}\")\n",
    "        \n",
    "        # Join all parts into a single text entry\n",
    "        recipe_text = \". \".join(recipe_parts)\n",
    "        recipes.append(recipe_text)\n",
    "    \n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b33c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools import tool\n",
    "\n",
    "recipes = gather_recipe_data(\"prajwaldongre/collection-of-recipes-around-the-world\")\n",
    "docs = [Document(page_content=recipe) for recipe in recipes]\n",
    "vector = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_recipes(query: str):\n",
    "  \"\"\"\n",
    "  Retrieves recipes based on a search query.\n",
    "  \"\"\"\n",
    "  return retriever.invoke(query)\n",
    "\n",
    "culinary_detective_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in world cuisines and culinary traditions.\"},\n",
    "    {\"role\": \"human\", \"content\": \"Given the following description, guess the most likely country and region. Answer in the format of [country], [region].\"},\n",
    "]\n",
    "culinary_detective = create_agent(model=llm, tools=[retrieve_recipes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_culinary_detective_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and region from the response\n",
    "    # return country, region\n",
    "    import re\n",
    "    \n",
    "    # Look for pattern [country], [region] or variations\n",
    "    bracket_pattern = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(bracket_pattern, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        region = match.group(2).strip()\n",
    "        return country, region\n",
    "    \n",
    "    # Fallback: look for two items separated by comma\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Look for comma-separated values\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                region = parts[1].strip().strip('[]\"\\'')\n",
    "                # Clean up common prefixes\n",
    "                for prefix in ['The answer is', 'Answer:', 'Country:', 'Region:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    region = region.replace(prefix, '').strip()\n",
    "                return country, region\n",
    "    \n",
    "    # Last resort: return empty strings\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672811",
   "metadata": {},
   "source": [
    "### ðŸ‘„Lingua Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad20b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua_locale_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in languages and cultural nuances.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given the following sentence, guess the which country's website it is from. Answer in the format of [country].\"},\n",
    "]\n",
    "lingua_locale = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lingua_locale_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and \"none\" from the response\n",
    "    # only the first field is used, the second is a dummy field to make the return type consistent\n",
    "    # return country, \"none\"\n",
    "    import re\n",
    "    \n",
    "    # Look for pattern [country] in brackets\n",
    "    bracket_pattern = r'\\[([^\\]]+)\\]'\n",
    "    match = re.search(bracket_pattern, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        return country, \"none\"\n",
    "    \n",
    "    # Fallback: look for country name in response\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Clean the line\n",
    "        clean_line = line.strip().strip('[]\"\\'')\n",
    "        # Remove common prefixes\n",
    "        for prefix in ['The answer is', 'Answer:', 'Country:', 'The country is', 'This is from']:\n",
    "            clean_line = clean_line.replace(prefix, '').strip()\n",
    "        \n",
    "        # If we have a short, cleaned line, it's likely the country\n",
    "        if clean_line and len(clean_line) < 50:\n",
    "            # Remove any trailing punctuation\n",
    "            clean_line = clean_line.rstrip('.,;:')\n",
    "            return clean_line, \"none\"\n",
    "    \n",
    "    # Last resort: return empty string\n",
    "    return \"\", \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3291f",
   "metadata": {},
   "source": [
    "## Answering questions\n",
    "This part includes how we load the questions and generate the prediction in desired format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoguesser(q: dict, print_raw_response=False) -> tuple[str, str]:\n",
    "    if q[\"type\"] == \"GlobalTrekker\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Paragraph: {q['paragraph']}\"}\n",
    "        messages, agent, extractor = global_trekker_messages, global_trekker, extract_global_trekker_answer\n",
    "    elif q[\"type\"] == \"CulinaryDetective\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Ingredients: {q['ingredient']}. Description: {q['description']}\"}\n",
    "        messages, agent, extractor = culinary_detective_messages, culinary_detective, extract_culinary_detective_answer\n",
    "    else: #q[\"type\"] == \"LinguaLocale\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Sentence: {q['sentence']}\"}\n",
    "        messages, agent, extractor = lingua_locale_messages, lingua_locale, extract_lingua_locale_answer\n",
    "\n",
    "    response_all = agent.invoke({\"messages\": messages + [query]})\n",
    "    response = response_all[\"messages\"][-1].content\n",
    "    if print_raw_response: print(f\"{q['type']}: {response_all}\")\n",
    "    return extractor(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45950e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Here, we load the examples questions. Public/private set will be in the same format\n",
    "dataset_name = \"example.jsonl\"\n",
    "questions = []\n",
    "with open(dataset_name, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        questions.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14091132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker: {'messages': [SystemMessage(content='You are an expert in world knowledge.', additional_kwargs={}, response_metadata={}, id='c0c4d44e-ea20-4aa5-8188-7c6839dafe8a'), HumanMessage(content='Given the following paragraph, guess the most likely country and city. Answer in the format of [country], [city].', additional_kwargs={}, response_metadata={}, id='d3a771c9-8b71-4b88-9f86-52fc4c0a6082'), HumanMessage(content=\"Paragraph: Picklesburgh is an annual event that takes place every July, named as a #1 Specialty Food Festival in the country. It celebrates the city's long pickle-making history, and the signature view is a giant Heinz pickle balloon floating above the venue.\", additional_kwargs={}, response_metadata={}, id='c9032bdf-d3e4-4b24-9018-f6718d0fc4e8'), AIMessage(content='[United States], [Pittsburgh]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 98, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CgsyIKQjuZCBaXXoAv9ZuIUiEs1pw', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--01d653c1-702c-427a-9fcf-77f3cda81ec2-0', usage_metadata={'input_tokens': 98, 'output_tokens': 10, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('United States', 'Pittsburgh')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run on one question\n",
    "# You might want to save the raw response for debugging answer formatting/extraction\n",
    "# If the extracted answer seems off, check the raw response instead of running inference repeatedly\n",
    "geoguesser(questions[0], print_raw_response=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Sample script to generate answers\n",
    "from tqdm import tqdm\n",
    "answers = []\n",
    "for q in tqdm(questions):\n",
    "    try:\n",
    "        country, category = geoguesser(q)\n",
    "        answers.append(f\"{q['type']}\\t{country}\\t{category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {q}: {e}\")\n",
    "        answers.append(f\"{q['type']}\\tUnknown\\tUnknown\")\n",
    "\n",
    "with open(\"example_output.txt\", \"w\") as f:\n",
    "    for answer in answers:\n",
    "        f.write(answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99774c89",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "This is how we calculate the scores on Gradescope (details subject to change, but the general logic will stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer in answer:\n",
    "        score = len(expectedAnswer) / len(answer)\n",
    "    return score\n",
    "\n",
    "def exact_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer == answer:\n",
    "        score = 1.0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker Average Score: 0.8606\n",
      "CulinaryDetective Average Score: 0.6000\n",
      "LinguaLocale Average Score: 0.8000\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append((q[\"type\"], q[\"country\"], q.get(\"city\", q.get(\"region\", \"\"))))\n",
    "with open(\"example_output.txt\", \"r\") as f:\n",
    "    preds = [line.split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "scores = {\"GlobalTrekker\": [], \"CulinaryDetective\": [], \"LinguaLocale\": []}\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(answers, preds):\n",
    "    assert q_type == p_type\n",
    "    country_score = soft_match(pred_country, exp_country)\n",
    "    category_score = 0.0\n",
    "    weights = [0.0, 0.0]\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        #  correct country -> 80%, correct country and city -> +20%\n",
    "        weights = [0.8, 0.2]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = soft_match(pred_place, exp_place)\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        # correct country -> 60%, correct country and region -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = exact_match(pred_place, exp_place)\n",
    "    else: # LinguaLocale\n",
    "        # correct country -> 60%, matched official language -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            category_score = 1.0\n",
    "        else: # incorrect country. language match works only if pred_country is a clean answer\n",
    "            exp_langs = list_of_countries.get(exp_country, [])\n",
    "            pred_langs = list_of_countries.get(pred_country, [])\n",
    "            if any(lang in exp_langs for lang in pred_langs):\n",
    "                category_score = 1.0\n",
    "\n",
    "    score = weights[0] * country_score + weights[1] * category_score\n",
    "    scores[q_type].append(score)\n",
    "\n",
    "for q_type, score_list in scores.items():\n",
    "    avg_score = sum(score_list) / len(score_list)\n",
    "    print(f\"{q_type} Average Score: {avg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Universal Environment",
   "language": "python",
   "name": "ml-universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
